{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>                                        Speech Emotion Recognition\n",
    "\n",
    "### <center>                                    What is Speech Emotion Recognition?\n",
    "\n",
    "#### Definition : \n",
    "\n",
    "Speech emotion recognition (SER) is the field of technology focused on identifying the emotional state of a speaker from their voice.  This goes beyond the words spoken and analyzes how they are spoken.\n",
    "\n",
    "#### How it Works :\n",
    "\n",
    "* **Speech Input**: Similar to standard speech recognition, the user's voice is recorded.\n",
    "* **Pre-processing**: The audio is prepared by removing noise and potentially isolating specific speech segments.\n",
    "* **Feature Extraction**: Crucial features related to emotions are extracted. These include:\n",
    "* **Prosodic features**: Pitch, intonation, volume, speaking rate, pauses\n",
    "* **Spectral Features**: Spectrum of the voice, MFCCs (emphasizing qualities similar to human perception)\n",
    "* **Voice Quality Features**: Jitter, shimmer (small variations in voice quality)\n",
    "* **Emotion Model**: A trained machine learning model (often using classification algorithms) takes these features and identifies the associated emotion.\n",
    "* **Emotion Output**: The system outputs the detected emotion, typically with probability or confidence scores (e.g., angry, happy, sad, neutral, etc.).\n",
    "\n",
    "#### Applications of Speech Emotion Recognition :\n",
    "\n",
    "* **Mental Health**: Potential uses in diagnosing and monitoring mental health conditions, detecting stress or depression.\n",
    "* **Customer Service**: Analyzing customer interactions in call centers to improve service and gauge satisfaction.\n",
    "* **Human-Computer Interaction**: Creating more responsive and emotionally intelligent virtual assistants and robots.\n",
    "* **Market Research**: Analyzing focus group responses or advertisement reception to understand emotional reactions.\n",
    "* **Game Design**: Developing adaptive games that change based on a player's emotional state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "import os\n",
    "import pandas as pd\n",
    "import IPython.display as ipd\n",
    "import plotly.express as px\n",
    "import sys\n",
    "from tensorflow.python.client import device_lib\n",
    "from audiomentations import Compose, AddGaussianNoise, PitchShift, TimeStretch, Shift, TimeMask, SpecFrequencyMask\n",
    "from joblib import Parallel, delayed\n",
    "import timeit\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether GPUs are available to use or not\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Ravdess Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ryerson Audio-Visual Database of Emotional Speech and Song (Ravdess) \n",
    "# (https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio)\n",
    "# Extract to the folder ./rav\n",
    "\n",
    "RAV = './rav/audio_speech_actors_01-24/'\n",
    "dir_list = os.listdir(RAV)\n",
    "\n",
    "emotion = []\n",
    "path = []\n",
    "for i in dir_list:\n",
    "    fname = os.listdir(RAV + i)\n",
    "    for f in fname:\n",
    "        part = f.split('.')[0].split('-')\n",
    "        # Extract emotion label\n",
    "        emotion.append(int(part[2]))\n",
    "\n",
    "        # Extract path\n",
    "        path.append(RAV + i + '/' + f)\n",
    "        \n",
    "RAV_df = pd.DataFrame(emotion)\n",
    "RAV_df = RAV_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\n",
    "RAV_df.columns = ['Emotions']\n",
    "RAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['Path'])],axis=1)\n",
    "RAV_df.Emotions.value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Crema Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crowd Sourced Emotional Multimodal Actors Dataset (CREMA-D)\n",
    "# (https://www.kaggle.com/datasets/ejlok1/cremad)\n",
    "# Extract to the folder ./cremad\n",
    "\n",
    "CREMA = \"./cremad/AudioWAV/\"\n",
    "crema_directory_list = os.listdir(CREMA)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in crema_directory_list:\n",
    "    # storing file paths\n",
    "    file_path.append(CREMA + file)\n",
    "    # storing file emotions\n",
    "    part=file.split('_')\n",
    "    if part[2] == 'SAD':\n",
    "        file_emotion.append('sad')\n",
    "    elif part[2] == 'ANG':\n",
    "        file_emotion.append('angry')\n",
    "    elif part[2] == 'DIS':\n",
    "        file_emotion.append('disgust')\n",
    "    elif part[2] == 'FEA':\n",
    "        file_emotion.append('fear')\n",
    "    elif part[2] == 'HAP':\n",
    "        file_emotion.append('happy')\n",
    "    elif part[2] == 'NEU':\n",
    "        file_emotion.append('neutral')\n",
    "    else:\n",
    "        file_emotion.append('Unknown')\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Crema_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Crema_df.head()\n",
    "print(Crema_df.Emotions.value_counts())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Tess Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toronto emotional speech set (TESS)\n",
    "# (https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess)\n",
    "# Extract to the folder /tess\n",
    "\n",
    "TESS = \"./tess/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data/\"\n",
    "tess_directory_list = os.listdir(TESS)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for dir in tess_directory_list:\n",
    "    directories = os.listdir(TESS + dir)\n",
    "    for file in directories:\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('_')[2]\n",
    "        if part=='ps':\n",
    "            file_emotion.append('surprise')\n",
    "        else:\n",
    "            file_emotion.append(part)\n",
    "        file_path.append(TESS + dir + '/' + file)\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Tess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Tess_df.head()\n",
    "print(Tess_df.Emotions.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Savee Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surrey Audio-Visual Expressed Emotion (SAVEE)\n",
    "# (https://www.kaggle.com/datasets/ejlok1/surrey-audiovisual-expressed-emotion-savee)\n",
    "# Extract to the folder ./savee\n",
    "\n",
    "SAVEE = \"./savee/ALL/\"\n",
    "savee_directory_list = os.listdir(SAVEE)\n",
    "\n",
    "file_emotion = []\n",
    "file_path = []\n",
    "\n",
    "for file in savee_directory_list:\n",
    "    file_path.append(SAVEE + file)\n",
    "    part = file.split('_')[1]\n",
    "    ele = part[:-6]\n",
    "    if ele=='a':\n",
    "        file_emotion.append('angry')\n",
    "    elif ele=='d':\n",
    "        file_emotion.append('disgust')\n",
    "    elif ele=='f':\n",
    "        file_emotion.append('fear')\n",
    "    elif ele=='h':\n",
    "        file_emotion.append('happy')\n",
    "    elif ele=='n':\n",
    "        file_emotion.append('neutral')\n",
    "    elif ele=='sa':\n",
    "        file_emotion.append('sad')\n",
    "    else:\n",
    "        file_emotion.append('surprise')\n",
    "        \n",
    "# dataframe for emotion of files\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# dataframe for path of files.\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "Savee_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "Savee_df.head()\n",
    "print(Savee_df.Emotions.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.concat([RAV_df, Crema_df, Tess_df, Savee_df], axis=0)\n",
    "main_df.to_csv(\"data_path.csv\",index=False)\n",
    "main_df.head()\n",
    "print(main_df.Emotions.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Visualization :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_emotions(df, title):\n",
    "    px_fig = px.histogram(df,\n",
    "                          x=\"Emotions\",\n",
    "                          color='Emotions',\n",
    "                          title=title)\n",
    "    px_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Ravdess Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_emotions(RAV_df, \"Emotion Visualization Of Rav Data Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Crema Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_emotions(Crema_df, \"Emotion Visualization Of Crema Data Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Tess Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_emotions(Tess_df, \"Emotion Visualization Of Tess Data Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Savee Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_emotions(Savee_df, \"Emotion Visualization Of Savee Data Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Combined Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_fig = px.histogram(main_df,\n",
    "                      x='Emotions',\n",
    "                      color='Emotions',\n",
    "                      title='Emotion Count')\n",
    "px_fig.update_layout(bargap=0.2)\n",
    "px_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_waveplot(data, sr, e):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.title('Waveplot for audio with {} emotion'.format(e), size=15)\n",
    "    librosa.display.waveshow(data, sr=sr)\n",
    "    plt.show()\n",
    "\n",
    "def create_spectrogram(data, sr, e):\n",
    "    # stft function converts the data into short term fourier transform\n",
    "    X = librosa.stft(data)\n",
    "    Xdb = librosa.amplitude_to_db(abs(X))\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)\n",
    "    librosa.display.specshow(Xdb,\n",
    "                             sr=sr,\n",
    "                             x_axis='time',\n",
    "                             y_axis='hz')   \n",
    "    #librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "def plot_magnitude_spectrum(signal, sr, ratio, e):\n",
    "    fft = np.abs(np.fft.fft(signal)) \n",
    "    frequency = np.linspace(0, sr, len(fft))\n",
    "    range = int(len(frequency) * ratio) \n",
    "    plt.plot(frequency[:range], fft[:range])\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.title(\"Magnitude Spectrum for audio with {} emption\".format(e))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion='fear'\n",
    "path = np.array(RAV_df.Path[RAV_df.Emotions==emotion])[1]\n",
    "data, sampling_rate = librosa.load(path)\n",
    "create_waveplot(data, sampling_rate, emotion)\n",
    "create_spectrogram(data, sampling_rate, emotion)\n",
    "plot_magnitude_spectrum(data, sampling_rate, 0.5, emotion)\n",
    "ipd.Audio(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Augmentation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual\n",
    "def noise(data):\n",
    "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
    "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
    "    return data\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(data, rate = rate)\n",
    "\n",
    "def shift(data):\n",
    "    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
    "    return np.roll(data, shift_range)\n",
    "\n",
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor)\n",
    "\n",
    "noise_aug = AddGaussianNoise(min_amplitude=0.0001,\n",
    "                             max_amplitude=0.0005,\n",
    "                             p=1)\n",
    "\n",
    "pitch_aug = PitchShift(min_semitones=-7.0,\n",
    "                       max_semitones=7.0,\n",
    "                       p=1)\n",
    "\n",
    "time_stretch_aug = TimeStretch(min_rate=0.8,\n",
    "                               max_rate=1.25,\n",
    "                               leave_length_unchanged=True,\n",
    "                               p=1)\n",
    "\n",
    "shift_aug = Shift(min_fraction=-0.1,\n",
    "                  max_fraction=0.1,\n",
    "                  rollover=True,\n",
    "                  p=1)\n",
    "\n",
    "\n",
    "\n",
    "time_mask = TimeMask(min_band_part=0.1,\n",
    "    max_band_part=0.15,\n",
    "    fade=True,\n",
    "    p=1.0,\n",
    ")\n",
    "\n",
    "aug = Compose([PitchShift(min_semitones=-7.0,\n",
    "                          max_semitones=7.0,\n",
    "                          p=0.5),\n",
    "               TimeStretch(min_rate=0.8,\n",
    "                          max_rate=1.25,\n",
    "                          leave_length_unchanged=True,\n",
    "                          p=0.5),\n",
    "               TimeMask(min_band_part=0.1,\n",
    "                        max_band_part=0.15,\n",
    "                        fade=True,\n",
    "                        p=0.5)\n",
    "                ])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Simple Audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an exampl file\n",
    "path = np.array(RAV_df.Path)[1]\n",
    "data, sample_rate = librosa.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "librosa.display.waveshow(y=data, sr=sample_rate)\n",
    "ipd.Audio(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Noise Injection :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = noise_aug(samples=data, sample_rate=sample_rate)\n",
    "plt.figure(figsize=(14,4))\n",
    "librosa.display.waveshow(y=x, sr=sample_rate)\n",
    "ipd.Audio(x, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Pitching :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pitch_aug(samples=data, sample_rate=sample_rate)\n",
    "plt.figure(figsize=(14,4))\n",
    "librosa.display.waveshow(y=x, sr=sample_rate)\n",
    "ipd.Audio(x, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Streching :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = time_stretch_aug(samples=data, sample_rate=sample_rate)\n",
    "plt.figure(figsize=(14,4))\n",
    "librosa.display.waveshow(y=x, sr=sample_rate)\n",
    "ipd.Audio(x, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Shifting :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = shift_aug(samples=data, sample_rate=sample_rate)\n",
    "plt.figure(figsize=(14,4))\n",
    "librosa.display.waveshow(y=x, sr=sample_rate)\n",
    "ipd.Audio(x, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Combining :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = aug(samples=data, sample_rate=sample_rate)\n",
    "plt.figure(figsize=(14,4))\n",
    "librosa.display.waveshow(y=x, sr=sample_rate)\n",
    "ipd.Audio(x, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Feature Extraction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data):\n",
    "    # ZCR\n",
    "    result = np.array([])\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
    "    result=np.hstack((result, zcr)) # stacking horizontally\n",
    "\n",
    "    # Chroma_stft\n",
    "    stft = np.abs(librosa.stft(data))\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, chroma_stft)) # stacking horizontally\n",
    "\n",
    "    # MFCC\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mfcc)) # stacking horizontally\n",
    "\n",
    "    # Root Mean Square Value\n",
    "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
    "    result = np.hstack((result, rms)) # stacking horizontally\n",
    "\n",
    "    # MelSpectogram\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
    "    result = np.hstack((result, mel)) # stacking horizontally\n",
    "    \n",
    "    return result\n",
    "\n",
    "def stack_augmented_layers(result, data, sample_rate, times):\n",
    "    for _ in range(times):\n",
    "        aug_data = aug(samples=data, sample_rate=sample_rate)  \n",
    "        res = extract_features(aug_data)  \n",
    "        result = np.vstack((result, res))   # stacking vertically\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def get_features(path, is_aug):\n",
    "    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
    "    res1 = extract_features(data)\n",
    "    result = np.array(res1)\n",
    "    \n",
    "    if (is_aug):\n",
    "        # Augementing data seperately\n",
    "        # time_mask_data = time_mask(sample_rate=sample_rate, samples=data)\n",
    "        # time_mask_data_feature = extract_features(time_mask_data)\n",
    "\n",
    "        # time_strech_data = time_stretch_aug(sample_rate=sample_rate, samples=data)\n",
    "        # time_stretch_data_feature = extract_features(time_strech_data)\n",
    "\n",
    "        # piched_data = pitch_aug(sample_rate=sample_rate, samples=data)\n",
    "        # pitched_data_feature = extract_features(piched_data)\\\n",
    "        \n",
    "        # result = np.vstack((result, time_mask_data_feature))\n",
    "        # result = np.vstack((result, time_stretch_data_feature))\n",
    "        # result = np.vstack((result, pitched_data_feature))\n",
    "\n",
    "        # Augment data composedly\n",
    "        # noise_data = noise(data)\n",
    "        # res2 = extract_features(noise_data)\n",
    "        # result = np.vstack((result, res2)) # stacking vertically\n",
    "\n",
    "        result = stack_augmented_layers(result=result,\n",
    "                                        data=data,\n",
    "                                        sample_rate=sample_rate,\n",
    "                                        times=2)\n",
    "        \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data Preperation :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split the data into train (70%) and temp (30%) using stratification on the 'Emotions' column\n",
    "train_df, temp_df = train_test_split(\n",
    "    main_df,\n",
    "    test_size=0.3,\n",
    "    stratify=main_df['Emotions'],  # Use the existing 'Emotions' column for stratification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Split the temp data into validation (20%) and test (10%) using stratification on the 'Emotions' column\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=1/3,  # 1/3 of 30% = 10%\n",
    "    stratify=temp_df['Emotions'],  # Use the 'Emotions' column for stratification\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Step 3: Print the sizes of each split\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Validation size:\", len(val_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "\n",
    "# Step 4: Check the distribution of labels in each split (optional)\n",
    "print(\"Train distribution:\\n\", train_df['Emotions'].value_counts())\n",
    "print(\"Validation distribution:\\n\", val_df['Emotions'].value_counts())\n",
    "print(\"Test distribution:\\n\", test_df['Emotions'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Saving feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_feature(df, is_aug):\n",
    "#     X, Y = [], []\n",
    "#     start = timeit.default_timer()\n",
    "#     for path, emotion in zip(df.Path, df.Emotions):\n",
    "#         feature = get_features(path, is_aug=is_aug)\n",
    "#         if (is_aug):\n",
    "#             for ele in feature:\n",
    "#                 X.append(ele)\n",
    "#                 Y.append(emotion)\n",
    "#         else:\n",
    "#             X.append(feature)\n",
    "#             Y.append(emotion)\n",
    "\n",
    "#     stop = timeit.default_timer()\n",
    "#     print('Normal way to get feature: ', stop - start)\n",
    "\n",
    "#     return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feature(path, emotion, is_aug):\n",
    "    features = get_features(path, is_aug)\n",
    "    X = []\n",
    "    Y = []\n",
    "    if (is_aug):\n",
    "        for ele in features:\n",
    "            X.append(ele)\n",
    "            Y.append(emotion)\n",
    "    else:\n",
    "        X.append(features)\n",
    "        Y.append(emotion)\n",
    "    return X, Y\n",
    "\n",
    "def extract_feature(df, is_aug):\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    paths = df.Path\n",
    "    emotions = df.Emotions\n",
    "    \n",
    "    results = Parallel(n_jobs=-1)(delayed(process_feature)(path, emotion, is_aug) for (path, emotion) in zip(paths, emotions))\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for result in results:\n",
    "        x, y = result\n",
    "        X.extend(x)\n",
    "        Y.extend(y)\n",
    "\n",
    "    stop = timeit.default_timer()\n",
    "\n",
    "    print('Get feature for training set in parallel manner: ', stop - start)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# Only apply data augumentation for training set\n",
    "X, Y = extract_feature(train_df, is_aug=True)  \n",
    "X_val, Y_val = extract_feature(val_df, is_aug=False)\n",
    "X_test, Y_test = extract_feature(test_df, is_aug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X), len(Y), len(X_val), len(Y_val), len(X_test), len(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = pd.DataFrame(X)\n",
    "Features['labels'] = Y\n",
    "Features.to_csv('features.csv', index=False)\n",
    "\n",
    "Features_val = pd.DataFrame(X_val)\n",
    "Features_val['labels'] = Y_val\n",
    "Features_val.to_csv('features_val.csv', index=False)\n",
    "\n",
    "Features_test = pd.DataFrame(X_test)\n",
    "Features_test['labels'] = Y_test\n",
    "Features_test.to_csv('features_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleansing data\n",
    "Features=Features.fillna(0)\n",
    "Features_val=Features_val.fillna(0)\n",
    "Features_test=Features_test.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Features.iloc[: ,:-1].values\n",
    "Y = Features['labels'].values\n",
    "\n",
    "X_val = Features_val.iloc[: ,:-1].values\n",
    "Y_val = Features_val['labels'].values\n",
    "\n",
    "X_test = Features_test.iloc[: ,:-1].values\n",
    "Y_test = Features_test['labels'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing label\n",
    "encoder = OneHotEncoder()\n",
    "Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n",
    "Y_val = encoder.fit_transform(np.array(Y_val).reshape(-1,1)).toarray()\n",
    "Y_test = encoder.fit_transform(np.array(Y_test).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "X_val = scaler.fit_transform(X_val)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dimension (depth = 1, voice channel)\n",
    "X = np.expand_dims(X, axis=2)\n",
    "X_val = np.expand_dims(X_val, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "X.shape, Y.shape, X_val.shape, Y_val.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Modeling & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(X.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(units=7, activation='softmax'))\n",
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=4, min_lr=0.0000001)\n",
    "history=model.fit(X, Y, batch_size=64, epochs=50, validation_data=(X_val, Y_val), callbacks=[rlrp])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of our model on test data : \" , model.evaluate(X_test,Y_test)[1]*100 , \"%\")\n",
    "\n",
    "epochs = [i for i in range(50)]\n",
    "fig , ax = plt.subplots(1,2)\n",
    "train_acc = history.history['accuracy']\n",
    "train_loss = history.history['loss']\n",
    "test_acc = history.history['val_accuracy']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "fig.set_size_inches(20,6)\n",
    "ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
    "ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n",
    "ax[0].set_title('Training & Testing Loss')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
    "ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n",
    "ax[1].set_title('Training & Testing Accuracy')\n",
    "ax[1].legend()  \n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(X_test)\n",
    "y_pred = encoder.inverse_transform(pred_test)\n",
    "y_test = encoder.inverse_transform(Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\n",
    "df['Predicted Labels'] = y_pred.flatten()\n",
    "df['Actual Labels'] = y_test.flatten()\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize = (12, 10))\n",
    "cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n",
    "sns.heatmap(cm, linecolor='white', cmap='Purples', linewidth=1, annot=True, fmt='')\n",
    "plt.title('Confusion Matrix', size=20)\n",
    "plt.xlabel('Predicted Labels', size=14)\n",
    "plt.ylabel('Actual Labels', size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
